{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader\n",
    "from data import NamesDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = Path.home() / \"mldata\" / \"makemore\" \n",
    "RUNROOT = Path.home() / \"mlruns\" / \"makemore\" / \"names\"\n",
    "EOT_TOKEN = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0,  5],\n",
       "         [ 0,  5, 13],\n",
       "         [ 5, 13, 13],\n",
       "         [13, 13,  1],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, 15],\n",
       "         [ 0, 15, 12],\n",
       "         [15, 12,  9],\n",
       "         [12,  9, 22]]),\n",
       " tensor([13, 13,  1,  0, 15, 12,  9, 22,  9]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_LEN = 3\n",
    "dataset = NamesDataset(DATAROOT / \"names.txt\", CONTEXT_LEN)\n",
    "dataset[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(traindl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3]) torch.Size([32])\n",
      "tensor([ 0,  0, 26]) tensor(8)\n"
     ]
    }
   ],
   "source": [
    "x, y = next(it)\n",
    "print(x.shape, y.shape)\n",
    "print(x[0], y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "Lets define the embedding layer using the nifty Embedding module. The $i^{th}$ row of the module is the embedding for the number $i$. E.g., `emb([5, 13, 9])` will output three rows, the first one will be `E[5]`, the second one will be `E[13]`, and the third one will be `E[9]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = t.nn.Embedding(27, EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1141,  0.3676],\n",
       "        [-1.1141,  0.3676],\n",
       "        [-0.9831, -0.6679]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1141,  0.3676])\n",
      "tensor([-1.1141,  0.3676])\n",
      "tensor([-1.1141,  0.3676])\n"
     ]
    }
   ],
   "source": [
    "print(emb.weight.data[x[0][0]])\n",
    "print(emb.weight.data[x[0][1]])\n",
    "print(emb.weight.data[x[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = emb(x)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1141,  0.3676],\n",
       "         [-1.1141,  0.3676],\n",
       "         [-0.9831, -0.6679]],\n",
       "\n",
       "        [[-0.3845, -0.1170],\n",
       "         [-0.1816,  0.9680],\n",
       "         [-0.8184, -0.0974]],\n",
       "\n",
       "        [[-1.1141,  0.3676],\n",
       "         [-1.1141,  0.3676],\n",
       "         [-1.1141,  0.3676]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "The embedding layer will convert each row of 3 integers into a matrix of $3 \\times 2$, with each row in the output being the vector representation of the corresponding character. We can combine these embedding outputs in different ways, in the lecture these 3 vectors are concatenated into a single vector. I can do this using the `view` method or the `flatten` method. Then each instance becomes a 6-element vector that is transformed by the linear layer into a 100-element vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 100\n",
    "fc1 = t.nn.Linear(in_features=CONTEXT_LEN * EMB_DIM, out_features=HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.flatten(start_dim=1)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1141,  0.3676, -1.1141,  0.3676, -0.9831, -0.6679],\n",
       "        [-0.3845, -0.1170, -0.1816,  0.9680, -0.8184, -0.0974],\n",
       "        [-1.1141,  0.3676, -1.1141,  0.3676, -1.1141,  0.3676]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1141,  0.3676, -1.1141,  0.3676, -0.9831, -0.6679],\n",
       "        [-0.3845, -0.1170, -0.1816,  0.9680, -0.8184, -0.0974],\n",
       "        [-1.1141,  0.3676, -1.1141,  0.3676, -1.1141,  0.3676]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.view(-1, CONTEXT_LEN * EMB_DIM)\n",
    "print(x2.shape)\n",
    "x2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = fc1(x2)\n",
    "x3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see if this works with a single instance. Note that I did not have add an additional batch-of-1 dimension to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(x[0]).view(-1, CONTEXT_LEN * EMB_DIM).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1(emb(x[0]).view(-1, CONTEXT_LEN * EMB_DIM)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linearity\n",
    "In the lecture non-linearity is given by the `tanh` function. This will squash each row of 100 floats into 100 floats where each element is between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n",
      "tensor(-1.8588, grad_fn=<MinBackward1>) tensor(1.6359, grad_fn=<MaxBackward1>)\n",
      "tensor(-0.9526, grad_fn=<MinBackward1>) tensor(0.9269, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x4 = t.tanh(x3)\n",
    "print(x4.shape)\n",
    "print(t.min(x3), t.max(x3))\n",
    "print(t.min(x4), t.max(x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Layer\n",
    "This is another linear layer that will output the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2 = t.nn.Linear(in_features=HIDDEN_DIM, out_features=dataset.len_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.len_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "logits = fc2(x4)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a single row -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2(t.tanh(fc1(emb(x[0]).view(-1, CONTEXT_LEN * EMB_DIM)))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "This model can be thought of as a multiclass classification problem where the 4th character is the \"label\" that the network is trying to predict. The 27 element vector output by the network is the probability distribution of each character. Cross entropy loss can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = t.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3244, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(logits, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Getting everything together we get the following model -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CharLangModel\n",
    "\n",
    "model = CharLangModel(\n",
    "    context_len=CONTEXT_LEN, \n",
    "    vocab_len=dataset.len_vocab(), \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    emb_dim=EMB_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(x)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4355, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When predicting from a single row, take care of the dims. A good practice is to send in a single row as a batch-of-1. In this model it is not neccessary. But the output here is a single batch of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x[0].shape)\n",
    "print(x[0].unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
