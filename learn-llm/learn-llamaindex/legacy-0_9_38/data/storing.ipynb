{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    body {\n",
    "        --vscode-font-family: \"Segoe UI\"\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 concepts here -\n",
    "  1. Key-Value Store\n",
    "  2. Document Store\n",
    "  3. Index Store\n",
    "  4. Vector Store\n",
    "  5. Graph Store\n",
    "  6. Different indexes (not to be confused with index stores) -\n",
    "      * Summary Index\n",
    "      * Vector Store Index\n",
    "      * Tree Index\n",
    "      * Keyword Table Index\n",
    "      * Knowledge Graph Index\n",
    "\n",
    "![storage_contex](./storage_context.png)\n",
    "\n",
    "### Key-Value Store\n",
    "This is the underlying physical storage layer. All data is stored as key-values. This abstraction is used by the document store and index store.\n",
    "\n",
    "### Document Store\n",
    "Real world content is ingested as documents, is parsed and chunked into text nodes and stored in the document store. Document stores use the underlying Key-Vaule to store to store the documents, possibly by using the doc ID as the key and the actual document as the value.\n",
    "\n",
    "### Index Store\n",
    "Directly from the [documentation](http://127.0.0.1:8000/module_guides/storing/index_stores.html) -\n",
    "> Index stores contain lightweight index metadata (i.e., additional state information created when building an index).\n",
    "\n",
    "TODO: Probe an index store to figure out what kind of metadata is stored here. Is it the `.metadata` of each node, or is it something else?\n",
    "\n",
    "### Vector Store\n",
    "This stores the embedding of each node. Any top level index that uses embeddings will use this store.\n",
    "\n",
    "### Graph Store\n",
    "This stores the so-called knowledge graph extracted from documents. This is similar to the vector store, in that only some indexes that deal with knowledge graphs will use this store.\n",
    "\n",
    "### Indexes\n",
    "Built on top of all these stores are different indexes. The indexes are just a way of arranging the nodes in the underlying doc store in some specific way that makes querying them for that specific purpose easy. I can then use any of the indexes as a query engine to query the underlying doc store. Some indexes like VectorStoreIndex and SummaryIndex use embeddings (optional in case of SummaryIndex) to aid the querying and some like SummaryIndex, KeywordTableIndex use keywords, etc. Indexes that use embeddings use the vector store in addition to the doc store and index store.\n",
    "\n",
    "Very good visual explanation given [here](http://127.0.0.1:8000/module_guides/indexing/index_guide.html).\n",
    "\n",
    "On the side there is also a Chat Store where we can store chat histories of different users. This does not seem to be related to any specific index.\n",
    "\n",
    "Just to make life interesting there are a bunch of objects that show up as \"vector stores\" that are actually document stores, index stores, and vector store index all rolled into one.\n",
    "\n",
    "This is not a very good layering of abstractions because it is leaking between different layers. It seems that the vector store was build to be expressly used by the vector store index and so on. Ideally, any component in the top layer abstraction should be able to use any other components in the immediately lower layer. But here its not like the summary index can use the graph store or anything like that.\n",
    "\n",
    "From the documentation -\n",
    "> Many vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This also means that you will not need to explicitly persist this data - this happens automatically.\n",
    "\n",
    "I tried this with Pinecone (see persist_pinecone.py) but it did not work. The documents do not get uploaded to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage import StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from llama_index import load_index_from_storage\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `persist_simple.py` for example of how to persist all the data (various datastores, indexes, etc.) to local filesystem. The following code loads this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKINGDIR = Path.home() / \"Desktop\" / \"temp\" / \"llama-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=str(WORKINGDIR/\"storage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a new conda environment, you can use the command `conda create --name ENVIRONMENT_NAME`. For example, to create a new environment named 'py35' with Python 3.5, you would use the command `conda create --name py35 python=3.5`.\n"
     ]
    }
   ],
   "source": [
    "index = load_index_from_storage(storage_context)\n",
    "query_engine = index.as_query_engine()\n",
    "resp = query_engine.query(\"How to create a new conda environment?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context == index.storage_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.vector_stores.simple.SimpleVectorStore object at 0x176eaa8c0>\n",
      "<llama_index.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x176eaaa10>\n",
      "<llama_index.storage.index_store.simple_index_store.SimpleIndexStore object at 0x176eaa980>\n"
     ]
    }
   ],
   "source": [
    "print(storage_context.vector_store)\n",
    "print(storage_context.docstore)\n",
    "print(storage_context.index_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list(storage_context.docstore.docs.values())\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(storage_context.index_store.index_structs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(storage_context.index_store.index_structs()[0].nodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embedding_dict', 'text_id_to_ref_doc_id', 'metadata_dict'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context.vector_store.to_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c20c64bd-2bd0-4113-8089-75c5f0ddf85e', '404380a9-84f3-4467-ac4a-f35406a73cee', '409dfddb-5dfd-4036-b660-7a468c59f16c', 'b7310392-fe92-465e-a507-a8a9895bf805', '60785f8e-c15f-4603-9d21-c62935b99d50', '2a88d256-873a-4f9b-8c50-5314aae60902', '389d6a7a-2c00-4737-af9e-f1ca9eb334b7', 'cecc4bb3-b402-44a2-9d1a-02a3ec77d5b3', '633aa7bd-e5c4-4ed5-a074-e5d7beea28ce', '1664aab1-e1e5-428b-b32f-9d5f9551277c', '8055a17e-8b3a-4379-bdd6-a6edb3090a6c', 'ce362ae2-d77c-4173-b5c2-d06e480d8b54', 'd79a0228-9be5-42dd-88ef-35f8a81b5f10', '643d1499-3781-4cc1-85e3-c0cc2fa1bee9', '137fa1e4-a239-4cdf-b824-14309e4c7ad8', 'e0c942f5-d4bf-44bf-9c75-0aa381c3b7e5', '32c8e556-e59a-4a21-aa1a-9a89a3c079ec', 'ea82c976-85f6-4b34-b584-a0b5cbe250ca', 'd9a0842c-1b28-4206-ad54-249e9d32a39a', '1c0e2754-1755-4e87-80c7-e60d05eaf463', '2cc505ba-96ca-437d-a671-4836e6615206', 'e4c2dbb0-0cfd-49d8-b4b1-a77e0a1ff3f2', 'fa14a282-f6f4-4bba-b03d-f36d74a20c50', '6ec692d3-4981-435a-987b-a30b4d0d62bf', 'fe3ae110-4f78-415c-bbd6-56242e9c22b8', 'f683048e-b76a-46f8-8c1b-b9f879266613', 'eb6d6f67-b960-4cdd-ae3c-431fd66b5618', '399a2a94-6272-42a4-a935-6bd0280b1c0f', '1e10b5eb-724b-40cd-88b3-b39a549058d9', 'c653169f-5eeb-4e34-a30f-8f7d2932e3cc', 'e0fdde1d-fedd-4646-8b50-d8cddbfc368e', '6d177357-8640-42fb-b7bd-0f922e628f87', '47f67ed2-34fb-4f24-a9b4-772dbedde0f3', '59fef44a-6ce4-4fc6-9244-3ce0dd4344a2', 'abf098d3-e37f-4759-86f1-9afb87019e64', '65f0b837-bc6a-4761-99b2-cb09b1bb544e', '208349a3-1f41-4e85-8147-e7d7dfdea1a5', 'f6d3fd65-bf81-47ee-b4fc-1d5e1f3361d1', 'd0fb8343-7228-47e6-adbd-37d3406d3004', 'c4f81981-a399-42f0-ad89-d3b1263fac5a'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context.vector_store.to_dict()[\"embedding_dict\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = len(storage_context.vector_store.to_dict()[\"embedding_dict\"][\"c20c64bd-2bd0-4113-8089-75c5f0ddf85e\"])\n",
    "emb_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default llama-index will use the local filesystem for persistence. But I can use any filesystem that implements the `fsspec.AbstractFileSystem` protocol. See [example](http://127.0.0.1:8000/module_guides/storing/save_load.html#using-a-remote-backend) for how to use S3 as the storage layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
