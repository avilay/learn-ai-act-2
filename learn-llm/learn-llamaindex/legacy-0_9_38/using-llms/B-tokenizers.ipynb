{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the choice of LLM model goes hand-in-hand with the choice of tokenizer. By default llama-index uses the `cl100k` tokenizer from `tiktoken` which is what is used by the default OpenAI model `gpt-3.5-turbo`. Why would llama-index need to use a tokenizer? All LLMs accept the actual text as input, not tokens. According to the llama-index [documentation](http://127.0.0.1:8000/module_guides/models/llms.html#a-note-on-tokenization) it uses the tokenizer for token counting. If that is all there is to it, then I don't really care about it if I don't get the most accurate count of tokens for each of my API calls. However, it has this gem of sentence in there as well - \n",
    "\n",
    "> If you change the LLM, you mane need to update this tokenizer to ensure accurate token counts, chunking, and prompting.\n",
    "\n",
    "Does it mess around the prompts and doc chunks before it sends them to the LLM? Who knows!! In any case, if I change the LLM, I should try out my test cases with and without its custom tokenizer.\n",
    "\n",
    "The tokenizer needs to be a callable that takes in a single string and returns a list of ints. Lets look at the default tokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 <class 'int'>\n",
      "[40, 3021, 311, 2068, 13]\n"
     ]
    }
   ],
   "source": [
    "toks = tokenize(\"I love to program.\")\n",
    "print(len(toks), type(toks[0]))\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can take any other tokenizer, e.g., from HuggingFace -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\").encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 315, 2016, 298, 3324, 2696, 28723]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I love to write code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can make any tokenizer default by calling the `llama_index.set_global_tokenizer(tokenize)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
