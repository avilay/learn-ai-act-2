{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files\n",
    "[Ref](http://localhost:8000/module_guides/loading/simpledirectoryreader.html#supported-file-types).\n",
    "`SimpleDirectoryReader` is meant as a prototyping tool and not to be used in production. Even so, it can read a whole bunch of file types as mentioned in the link above. Here are some useful ones from that link -\n",
    "  * .csv\n",
    "  * .docx\n",
    "  * .epub\n",
    "  * .ipynb\n",
    "  * .jpeg, .jpg\n",
    "  * .md\n",
    "  * .mp3, .mp4\n",
    "  * .pdf\n",
    "  * .png\n",
    "  * .ppt, .pptm, .pptx\n",
    "\n",
    "For some reason `SimpleDirectoryReader` does not read JSON. For that check out [JSONLoader](https://llamahub.ai/l/readers/llama-index-readers-json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 5/5 [00:00<00:00, 10.37file/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = SimpleDirectoryReader(\"./data\", recursive=True).load_data(show_progress=True)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsmap = defaultdict(list)\n",
    "for doc in docs:\n",
    "    docsmap[doc.metadata[\"file_name\"]].append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents\n",
    "All data loaders, even the third party ones in Llama-Hub have a `load_data()` method that will return a list of `Document` objects as we have seen above. For the most part the `Document` object will look like this -\n",
    "\n",
    "```\n",
    "Document(\n",
    "  id_='c33dc7aa-e994-48a4-8dcc-e76c163a2ca3', \n",
    "  embedding=None, \n",
    "  metadata={\n",
    "      'file_path': 'data/txts/paul_graham_essay.txt', \n",
    "      'file_name': 'paul_graham_essay.txt', \n",
    "      'file_type': 'text/plain', \n",
    "      'file_size': 75042, \n",
    "      'creation_date': '2024-04-10', \n",
    "      'last_modified_date': '2024-03-21', \n",
    "      'last_accessed_date': '2024-04-10'\n",
    "  }, \n",
    "  excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], \n",
    "  excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], \n",
    "  relationships={}, \n",
    "  text='...', \n",
    "  start_char_idx=None, \n",
    "  end_char_idx=None, \n",
    "  text_template='{metadata_str}\\n\\n{content}', \n",
    "  metadata_template='{key}: {value}', \n",
    "  metadata_seperator='\\n'\n",
    ")\n",
    "```\n",
    "\n",
    "In the call in the preceeding cell, we got 1 more document than the number of files in our data directory. This is because if the actual document has multiple pages, then each page will have its own `Document` object along with `page_label` metadata. In the example above, we loaded a \"single page\" text file and image file, so that resulted in single objects for each. The `conda-cheatsheet.pdf` is a 2-page document, so each of its page got its own object. These objects have a `page_label` metadata that tells us the page number.\n",
    "\n",
    "The `metadata` property has a already a lot of useful information by default. There is a way to add more to it when loading the doc. \n",
    "\n",
    "It is possible to create `Document` objects directly as well. A nice convenience method on `Document` is the `.example()` method that will generate some sample text for quick prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='18982618-cbda-4b22-a4ca-2866b8e7bef0', embedding=None, metadata={'file_path': 'data/txts/ozymandias.txt', 'file_name': 'ozymandias.txt', 'file_type': 'text/plain', 'file_size': 637, 'creation_date': '2024-04-13', 'last_modified_date': '2024-04-10', 'last_accessed_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='I met a traveller from an antique land,\\nWho said—“Two vast and trunkless legs of stone\\nStand in the desert. . . . Near them, on the sand,\\nHalf sunk a shattered visage lies, whose frown,\\nAnd wrinkled lip, and sneer of cold command,\\nTell that its sculptor well those passions read\\nWhich yet survive, stamped on these lifeless things,\\nThe hand that mocked them, and the heart that fed;\\nAnd on the pedestal, these words appear:\\nMy name is Ozymandias, King of Kings;\\nLook on my Works, ye Mighty, and despair!\\nNothing beside remains. Round the decay\\nOf that colossal Wreck, boundless and bare\\nThe lone and level sands stretch far away.”\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsmap[\"ozymandias.txt\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading PDFs\n",
    "I'll need to install `pypdf` package to read pdfs.\n",
    "\n",
    "Even though `SimpleDirectoryReader` can read simple PDFs it is not very good at even slightly stylized PDF as can be seen when reading the table in the conda cheat sheet.\n",
    "\n",
    "```\n",
    "Document(\n",
    "  id_='e999ff9c-d654-45da-80fc-d0baf2d5edd3', \n",
    "  embedding=None, \n",
    "  metadata={\n",
    "    'page_label': '1', \n",
    "    'file_name': 'conda-cheatsheet.pdf', \n",
    "    'file_path': 'data/pdfs/conda-cheatsheet.pdf', \n",
    "    'file_type': 'application/pdf', \n",
    "    'file_size': 299120, \n",
    "    'creation_date': '2024-04-10', \n",
    "    'last_modified_date': '2024-04-10', \n",
    "    'last_accessed_date': '2024-04-10'\n",
    "  }, \n",
    "  excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], \n",
    "  excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], \n",
    "  relationships={}, \n",
    "  text='...',\n",
    "  start_char_idx=None, \n",
    "  end_char_idx=None, \n",
    "  text_template='{metadata_str}\\n\\n{content}', \n",
    "  metadata_template='{key}: {value}', \n",
    "  metadata_seperator='\\n'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDA CHEAT SHEET\n",
      "Command line package and environment manager\n",
      "Learn to use conda in 30 minutes at bit.ly/tryconda TIP: Anaconda Navigator is a graphical interface to use conda.  \n",
      "Double-click the Navigator icon on your desktop or in a Terminal or at \n",
      "the Anaconda prompt, type anaconda-navigator\n",
      "CONTINUED ON BACK →conda info\n",
      "conda update conda\n",
      "conda install PACKAGENAME  \n",
      "spyder \n",
      "conda update PACKAGENAME\n",
      "COMMANDNAME --help  \n",
      "conda install --helpConda basicsVerify conda is installed, check version numberUpdate conda to the current version\n",
      "Install a package included in Anaconda\n",
      "Run a package after install, example Spyder*Update any installed programCommand line help\n",
      " \n",
      "*Must be installed and have a deployable command,  \n",
      "usually PACKAGENAME\n",
      "conda create --name py35 python=3.5 \n",
      "WINDOWS:    activate py35  \n",
      "LINUX, macOS: source activate py35conda env list\n",
      " \n",
      "conda create --clone py35 --name py35-2\n",
      "conda listconda list --revisionsconda install --revision 2\n",
      "conda list --explicit > bio-env.txt\n",
      "conda env remove --name bio-envWINDOWS: deactivate  \n",
      "macOS, LINUX: source deactivateconda env create --file bio-env.txt \n",
      "conda create --name bio-env biopython\n",
      "Use conda to search for a package\n",
      "See list of all packages in Anacondaconda search PACKAGENAME\n",
      "https://docs.anaconda.com/anaconda/packages/pkg-docsFinding conda packagesUsing environments\n",
      "Create a new environment named py35, install Python 3.5\n",
      "Activate the new environment to use it\n",
      " \n",
      "Get a list of all my environments, active environment is shown with *\n",
      "Make exact copy of an environmentList all packages and versions installed in active environment\n",
      "List the history of each change to the current environment\n",
      "Restore environment to a previous revisionSave environment to a text ﬁleDelete an environment and everything in it\n",
      "Deactivate the current environment \n",
      " \n",
      "Create environment from a text ﬁle\n",
      "Stack commands: create a new environment, name \n",
      "it bio-env and install the biopython package\n"
     ]
    }
   ],
   "source": [
    "print(docsmap[\"conda-cheatsheet.pdf\"][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conda create --name py34 python=3.4\n",
      "Windows:   activate py34\n",
      "Linux, macOS:  source activate py34\n",
      "Windows:  where python\n",
      "Linux, macOS: which -a pythonpython --versionInstalling and updating packages  \n",
      "Install a new package (Jupyter Notebook)  \n",
      "in the active environment\n",
      "Run an installed package (Jupyter Notebook)\n",
      "Install a new package (toolz) in a different environment \n",
      "(bio-env)  \n",
      "Update a package in the current environmentInstall a package (boltons) from a speciﬁc channel \n",
      "(conda-forge)\n",
      "Install a package directly from PyPI into the current active \n",
      "environment using pip \n",
      "Remove one or more packages (toolz, boltons)  \n",
      "from a speciﬁc environment (bio-env)\n",
      "Specifying version numbers\n",
      "Ways to specify a package version number for use with conda create or conda install commands, and in meta.yaml ﬁles.\n",
      "Constraint type Specification Result\n",
      "Fuzzy numpy=1.11 1.11.0, 1.11.1, 1.11.2, 1.11.18 etc.\n",
      "Exact numpy==1.11 1.11.0\n",
      "Greater than or equal to \"numpy>=1.11\" 1.11.0 or higher\n",
      "OR \"numpy=1.11.1|1.11.3\" 1.11.1, 1.11.3\n",
      "AND \"numpy>=1.8,<2\" 1.8, 1.9, not 2.0\n",
      "NOTE:  Quotation marks must be used when your speciﬁcation contains a space or any of these characters:  >  <  |  *\n",
      "Free Community Support  \n",
      "Online Documentation  \n",
      "Command Reference  \n",
      "Paid Support Options  \n",
      "Anaconda Onsite Training Courses  \n",
      "Anaconda Consulting Servicesgroups.google.com/a/continuum.io/forum/#!forum/conda  \n",
      "conda.io/docs  \n",
      "conda.io/docs/commands  \n",
      "anaconda.com/support  \n",
      "anaconda.com/training  \n",
      "anaconda.com/consultingMORE RESOURCES\n",
      "Follow us on Twitter @anacondainc and join the #AnacondaCrew!\n",
      "Connect with other talented, like-minded data scientists and developers while contributing to the open source movement. Visit anaconda.com/community\n",
      "anaconda.com · info@anaconda.com ·  512-776-10668/20/2017 conda cheat sheet Version 4.3.24Managing multiple versions of Python\n",
      "Install different version of Python in  \n",
      "a new environment named py34 \n",
      "Switch to the new environment that has  \n",
      "a different version of Python \n",
      "Show the locations of all versions of Python that are \n",
      "currently in the path  \n",
      "NOTE: The ﬁrst version of Python in the list will be executed.\n",
      "Show version information for the current active Pythonconda install jupyter\n",
      "jupyter-notebook\n",
      "conda install --name bio-env toolz\n",
      "conda update scikit-learn  \n",
      " conda install --channel conda-forge \n",
      "boltons \n",
      " pip install boltons\n",
      "conda remove --name bio-env toolz boltons\n"
     ]
    }
   ],
   "source": [
    "print(docsmap[\"conda-cheatsheet.pdf\"][1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'conda-cheatsheet.pdf',\n",
       " 'file_path': 'data/pdfs/conda-cheatsheet.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 299120,\n",
       " 'creation_date': '2024-04-13',\n",
       " 'last_modified_date': '2024-04-10',\n",
       " 'last_accessed_date': '2024-04-13'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsmap[\"conda-cheatsheet.pdf\"][0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '2',\n",
       " 'file_name': 'conda-cheatsheet.pdf',\n",
       " 'file_path': 'data/pdfs/conda-cheatsheet.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 299120,\n",
       " 'creation_date': '2024-04-13',\n",
       " 'last_modified_date': '2024-04-10',\n",
       " 'last_accessed_date': '2024-04-13'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsmap[\"conda-cheatsheet.pdf\"][1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Images\n",
    "I'll need to install `Pillow` package to read images.\n",
    "\n",
    "Even though `SimpleDirectoryReader` supports reading images, the `ImageDocument` object produced is not very useful. I'd have expected the binary content of the image as part of the object, just like the text content is, but I don't see it. Here is what the `ImageDocument` will look like -\n",
    "\n",
    "```\n",
    "ImageDocument(\n",
    "    id_='cab49e8c-c19f-45ad-ba51-dfed099a3156', \n",
    "    embedding=None, \n",
    "    metadata={\n",
    "        'file_path': 'data/imgs/python.png', \n",
    "        'file_name': 'python.png', \n",
    "        'file_type': 'image/png', \n",
    "        'file_size': 8758, \n",
    "        'creation_date': '2024-04-10', \n",
    "        'last_modified_date': '2023-11-02', \n",
    "        'last_accessed_date': '2024-04-10'\n",
    "    }, \n",
    "    excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'],\n",
    "    excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], \n",
    "    relationships={}, \n",
    "    text='', \n",
    "    start_char_idx=None, \n",
    "    end_char_idx=None, \n",
    "    text_template='{metadata_str}\\n\\n{content}', \n",
    "    metadata_template='{key}: {value}', \n",
    "    metadata_seperator='\\n', \n",
    "    image=None, \n",
    "    image_path='data/imgs/python.png', \n",
    "    image_url=None, \n",
    "    image_mimetype=None, \n",
    "    text_embedding=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDocument(id_='0307419d-da34-435c-9094-b277c0987937', embedding=None, metadata={'file_path': 'data/imgs/python.png', 'file_name': 'python.png', 'file_type': 'image/png', 'file_size': 8758, 'creation_date': '2024-04-10', 'last_modified_date': '2023-11-02', 'last_accessed_date': '2024-04-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', image=None, image_path='data/imgs/python.png', image_url=None, image_mimetype=None, text_embedding=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsmap[\"python.png\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing Documents\n",
    "When creating `Document` directly, we lose all the automatic metadata, it has to be set manually. Metadata is mostly used by the embedding model and the LLM. They use the, `text_template`, `metadata_template`, and `metadata_separator` to parse out the metadata from the actual content when calling `Document.get_content()`. We can control which metadata fields will be seen or not by either of these models via the `excluded_embed_metadata_keys` and `excluded_llm_metadata_keys`.\n",
    "\n",
    "Documents and their metadata can be customized pretty extensively as shown in the [documentation](http://localhost:8000/module_guides/loading/documents_and_nodes/usage_documents.html#customizing-documents). Below are some useful examples of customizing and using the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='54057212-c168-4fc4-a266-b4a6c232f30f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I met a traveller from an antique land,\\nWho said—“Two vast and trunkless legs of stone\\nStand in the desert. . . . Near them, on the sand,\\nHalf sunk a shattered visage lies, whose frown,\\nAnd wrinkled lip, and sneer of cold command,\\nTell that its sculptor well those passions read\\nWhich yet survive, stamped on these lifeless things,\\nThe hand that mocked them, and the heart that fed;\\nAnd on the pedestal, these words appear:\\nMy name is Ozymandias, King of Kings;\\nLook on my Works, ye Mighty, and despair!\\nNothing beside remains. Round the decay\\nOf that colossal Wreck, boundless and bare\\nThe lone and level sands stretch far away.”\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/txts/ozymandias.txt\") as fin:\n",
    "    doc = Document(text=fin.read())\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.metadata[\"filepath\"] = \"data/txts/ozymandias.txt\"\n",
    "doc.metadata[\"file_name\"] = \"ozymandias.txt\"\n",
    "doc.metadata[\"file_type\"] = \"text/plain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='54057212-c168-4fc4-a266-b4a6c232f30f', embedding=None, metadata={'filepath': 'data/txts/ozymandias.txt', 'file_name': 'ozymandias.txt', 'file_type': 'text/plain'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='I met a traveller from an antique land,\\nWho said—“Two vast and trunkless legs of stone\\nStand in the desert. . . . Near them, on the sand,\\nHalf sunk a shattered visage lies, whose frown,\\nAnd wrinkled lip, and sneer of cold command,\\nTell that its sculptor well those passions read\\nWhich yet survive, stamped on these lifeless things,\\nThe hand that mocked them, and the heart that fed;\\nAnd on the pedestal, these words appear:\\nMy name is Ozymandias, King of Kings;\\nLook on my Works, ye Mighty, and despair!\\nNothing beside remains. Round the decay\\nOf that colossal Wreck, boundless and bare\\nThe lone and level sands stretch far away.”\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='0132bd28-e985-416e-aad2-d3a482aab93a', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\nThey are pre-trained on large amounts of publicly available data.\\nHow do we best augment LLMs with our own private data?\\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\nProposed Solution\\nThat\\'s where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\nProvides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\nProvides an advanced retrieval/query interface over your data:\\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\nAllows easy integrations with your outer application framework\\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\\nLlamaIndex provides tools for both beginner users and advanced users.\\nOur high-level API allows beginner users to use LlamaIndex to ingest and\\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\\nreranking modules), to fit their needs.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document.example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file_path',\n",
       " 'file_name',\n",
       " 'file_type',\n",
       " 'file_size',\n",
       " 'creation_date',\n",
       " 'last_modified_date',\n",
       " 'last_accessed_date']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docsmap[\"ozymandias.txt\"][0].metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file_name',\n",
       " 'file_type',\n",
       " 'file_size',\n",
       " 'creation_date',\n",
       " 'last_modified_date',\n",
       " 'last_accessed_date']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docsmap[\"ozymandias.txt\"][0].excluded_embed_metadata_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file_name',\n",
       " 'file_type',\n",
       " 'file_size',\n",
       " 'creation_date',\n",
       " 'last_modified_date',\n",
       " 'last_accessed_date']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docsmap[\"ozymandias.txt\"][0].excluded_llm_metadata_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{metadata_str}\n",
      "\n",
      "{content}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{key}: {value}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "print(docsmap[\"ozymandias.txt\"][0].text_template)\n",
    "print(\"-\" * 100)\n",
    "print(docsmap[\"ozymandias.txt\"][0].metadata_template)\n",
    "print(\"-\" * 100)\n",
    "print(docsmap[\"ozymandias.txt\"][0].metadata_seperator)  # If there are multiple metadata keys as part of full content.\n",
    "print(\"-\" * 100, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: data/txts/ozymandias.txt\n",
      "\n",
      "I met a traveller from an antique land,\n",
      "Who said—“Two vast and trunkless legs of stone\n",
      "Stand in the desert. . . . Near them, on the sand,\n",
      "Half sunk a shattered visage lies, whose frown,\n",
      "And wrinkled lip, and sneer of cold command,\n",
      "Tell that its sculptor well those passions read\n",
      "Which yet survive, stamped on these lifeless things,\n",
      "The hand that mocked them, and the heart that fed;\n",
      "And on the pedestal, these words appear:\n",
      "My name is Ozymandias, King of Kings;\n",
      "Look on my Works, ye Mighty, and despair!\n",
      "Nothing beside remains. Round the decay\n",
      "Of that colossal Wreck, boundless and bare\n",
      "The lone and level sands stretch far away.”\n"
     ]
    }
   ],
   "source": [
    "print(docsmap[\"ozymandias.txt\"][0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: data/txts/ozymandias.txt\n",
      "\n",
      "I met a traveller from an antique land,\n",
      "Who said—“Two vast and trunkless legs of stone\n",
      "Stand in the desert. . . . Near them, on the sand,\n",
      "Half sunk a shattered visage lies, whose frown,\n",
      "And wrinkled lip, and sneer of cold command,\n",
      "Tell that its sculptor well those passions read\n",
      "Which yet survive, stamped on these lifeless things,\n",
      "The hand that mocked them, and the heart that fed;\n",
      "And on the pedestal, these words appear:\n",
      "My name is Ozymandias, King of Kings;\n",
      "Look on my Works, ye Mighty, and despair!\n",
      "Nothing beside remains. Round the decay\n",
      "Of that colossal Wreck, boundless and bare\n",
      "The lone and level sands stretch far away.”\n"
     ]
    }
   ],
   "source": [
    "print(docsmap[\"ozymandias.txt\"][0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "Typically a single document (or a single page of multi-page document) is represented by a single `Document` object. But this is not neccessarily how I might want to save the document to the index. A typical use case is to break the document into multiple chunks and then index the chunks individually. This makes the retrieval more precise. A document chunk is called a **Node** in Llama-Index. The type of a document chunk aka Node is actually `TextNode`. Confusingly enough there is a also a `Node` class in this class family.\n",
    "\n",
    "![doc-class](./doc-class.png)\n",
    "\n",
    "But for the most part, when the documentation is referring to a \"Node\" it is referring to `TextNode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of document is <class 'llama_index.schema.Document'>, type of node is a <class 'llama_index.schema.TextNode'>\n",
      "1 documents were split into 1 nodes\n"
     ]
    }
   ],
   "source": [
    "mddocs = SimpleDirectoryReader(\"./data/mds/\").load_data()\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(mddocs)\n",
    "print(f\"Type of document is {type(mddocs[0])}, type of node is a {type(nodes[0])}\")\n",
    "print(f\"{len(mddocs)} documents were split into {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = parser.get_nodes_from_documents([docsmap[\"sunset.md\"][0]])\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.file import SimpleFileNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'extension'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m parser \u001b[38;5;241m=\u001b[39m SimpleFileNodeParser()\n\u001b[0;32m----> 2\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdocsmap\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msunset.md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(nodes)\n",
      "File \u001b[0;32m~/projects/cloned/llama_index/llama_index/node_parser/interface.py:69\u001b[0m, in \u001b[0;36mNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m doc_id_to_document \u001b[38;5;241m=\u001b[39m {doc\u001b[38;5;241m.\u001b[39mid_: doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents}\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     67\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mNODE_PARSING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mDOCUMENTS: documents}\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m---> 69\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     73\u001b[0m             node\u001b[38;5;241m.\u001b[39mref_doc_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m node\u001b[38;5;241m.\u001b[39mref_doc_id \u001b[38;5;129;01min\u001b[39;00m doc_id_to_document\n\u001b[1;32m     75\u001b[0m         ):\n",
      "File \u001b[0;32m~/projects/cloned/llama_index/llama_index/node_parser/file/simple_file.py:68\u001b[0m, in \u001b[0;36mSimpleFileNodeParser._parse_nodes\u001b[0;34m(self, nodes, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m documents_with_progress \u001b[38;5;241m=\u001b[39m get_tqdm_iterable(\n\u001b[1;32m     64\u001b[0m     nodes, show_progress, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing documents into nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents_with_progress:\n\u001b[0;32m---> 68\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m FILE_NODE_PARSERS:\n\u001b[1;32m     70\u001b[0m         parser \u001b[38;5;241m=\u001b[39m FILE_NODE_PARSERS[ext](\n\u001b[1;32m     71\u001b[0m             include_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_metadata,\n\u001b[1;32m     72\u001b[0m             include_prev_next_rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_prev_next_rel,\n\u001b[1;32m     73\u001b[0m             callback_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager,\n\u001b[1;32m     74\u001b[0m         )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'extension'"
     ]
    }
   ],
   "source": [
    "parser = SimpleFileNodeParser()\n",
    "nodes = parser.get_nodes_from_documents([docsmap[\"sunset.md\"][0]])\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.flat_reader import FlatReader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_docs = FlatReader().load_data(Path(\"./data/mds/books.md\"))\n",
    "len(md_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'books.md', 'extension': '.md'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = parser.get_nodes_from_documents(md_docs)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='1e939504-e9e5-4100-83c8-a345682d64fe', embedding=None, metadata={'Header 1': 'Books', 'filename': 'books.md', 'extension': '.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='633e0cc9-602d-4150-9380-9deb46cf4aee', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filename': 'books.md', 'extension': '.md'}, hash='54f6dbc1566612a87ec0f9d2266d7502d2de2e5ad8e4bae9856c1c35190713ea'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='57763c71-15b2-490e-8433-9a981296b8ed', node_type=<ObjectType.TEXT: '1'>, metadata={'Header 1': 'Books', 'Header 2': 'Business', 'filename': 'books.md', 'extension': '.md'}, hash='229bcfd44b3585334ce8c3fa2b6c8a3c9e398d6cdaa60130274df4d751fe8d11')}, text='Books\\n\\n==Highlighted== books are books I want to read next.', start_char_idx=2, end_char_idx=61, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 - \n",
      "Books\n",
      "\n",
      "==Highlighted== books are books I want to read next.\n",
      "Chunk 1 - \n",
      "Business\n",
      "\n",
      "[FREE - The Future of a Radical Price](https://www.amazon.com/FREE-Future-Radical-Price/dp/B0055PK366/)\n",
      "\n",
      "[Seeing What's Next](https://www.amazon.com/Seeing-Whats-Next-Theories-Innovation/dp/1591391857/)\n",
      "\n",
      "[Measure What Matters](https://www.amazon.com/Measure-What-Matters-Google-Foundation/dp/0525536221/)\n",
      "\n",
      "[Crossing the Chasm](https://www.amazon.com/Crossing-Chasm-3rd-Disruptive-Mainstream/dp/0062356852/)\n",
      "\n",
      "==[Hooked: How to Build Habit-Forming Products](https://www.amazon.com/Hooked-How-Build-Habit-Forming-Products/dp/0241184835/)==\n",
      "\n",
      "[Reinventing Organizations](https://www.amazon.com/Reinventing-Organizations-Frederic-Laloux/dp/2960133501/)\n",
      "\n",
      "[The Truth About New Rules of Business Writing](https://www.amazon.com/Truth-About-Rules-Business-Writing-ebook/dp/B0031PXEGS/)\n",
      "\n",
      "[The Staff Engineer's Path](https://www.amazon.com/Staff-Engineers-Path-Individual-Contributors/dp/1098118731/)\n",
      "Chunk 2 - \n",
      "Popular Science\n",
      "\n",
      "[Astrophysics for People in a Hurry](https://www.amazon.com/Astrophysics-People-Hurry-deGrasse-Tyson/dp/0393609391/)\n",
      "\n",
      "[The Elegant Universe](https://www.amazon.com/Elegant-Universe-Superstrings-Dimensions-Ultimate/dp/0393058581/)\n",
      "\n",
      "[Weapons of Math Destruction](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418831/)\n",
      "\n",
      "[The Book of Why](https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X/)\n",
      "\n",
      "The Machine Learning Yearning\n",
      "\n",
      "[The Art of Doing Science and Engineering](https://www.amazon.com/Art-Doing-Science-Engineering-Learning/dp/1732265178/)\n",
      "\n",
      "[Thinking Fast and Slow](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/)\n",
      "\n",
      "[The Existential Pleasures of Engineering](https://www.amazon.com/Existential-Pleasures-Engineering-Thomas-Dunne/dp/0312141041/)\n",
      "\n",
      "[A Mind at Play](https://www.amazon.com/Mind-Play-Shannon-Invented-Information/dp/1476766681/)\n",
      "Chunk 3 - \n",
      "Mental Strategies\n",
      "\n",
      "==[The Power of Habit](https://www.amazon.com/Power-Habit-What-Life-Business/dp/1400069289/)==\n",
      "\n",
      "[Change the Way You See Everything](https://www.amazon.com/Seeing-Whats-Next-Theories-Innovation/dp/1591391857/)\n",
      "\n",
      "[Thinkertoys](https://www.amazon.com/Thinkertoys-Handbook-Creative-Thinking-Techniques-2nd/dp/1580087736/)\n",
      "\n",
      "[the dip](https://www.amazon.com/Dip-Little-Book-Teaches-Stick/dp/1591841666/)\n",
      "\n",
      "==[Atomic Habits](https://www.amazon.com/Atomic-Habits-Proven-Build-Break/dp/0735211299/)==\n",
      "\n",
      "[The Asshole Survival Guide](https://www.amazon.com/Asshole-Survival-Guide-People-Treat/dp/1328695913/)\n",
      "Chunk 4 - \n",
      "Philosophy\n",
      "\n",
      "==[Meditations: A New Translation]( https://www.amazon.com/Meditations-New-Translation-Marcus-Aurelius/dp/0812968255/)==\n",
      "\n",
      "[Zen Golf: Mastering the Mental Game](https://www.amazon.com/Zen-Golf-Mastering-Mental-Game/dp/0385504462/)\n",
      "\n",
      "[If you meet Buddha on the road, kill him](https://www.amazon.com/Meet-Buddha-Pilgrimage-Psychotherapy-Patients/dp/0831400323/)\n",
      "\n",
      "==[The Bhagvad Gita](https://www.amazon.com/Bhagavad-Gita-Comes-Alive-Translation/dp/097836614X/)==\n",
      "Chunk 5 - \n",
      "Math\n",
      "\n",
      "[Linear Algebra and Learning from Data](https://www.amazon.com/Linear-Algebra-Learning-Gilbert-Strang/dp/0692196382/)\n",
      "\n",
      "[Applied Logistic Regression](https://www.amazon.com/Applied-Logistic-Regression-Probability-Statistics/dp/0471356328/)\n",
      "\n",
      "[A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811)\n",
      "\n",
      "[Information Theory, Inference, and Learning Algorithms](https://www.amazon.com/Information-Theory-Inference-Learning-Algorithms/dp/0521642981/)\n",
      "\n",
      "[Spreadsheet Modeling and Decision Analysis](https://www.amazon.com/Spreadsheet-Modeling-Decision-Analysis-Introduction/dp/0357132092/)\n",
      "\n",
      "[The Little Handbook of Statistical Practices](https://www.amazon.com/Little-Handbook-Statistical-Practice-ebook/dp/B00847SM6A)\n",
      "Chunk 6 - \n",
      "Deep Learning\n",
      "\n",
      "[Grokking Deep Reinforcement Learning](https://www.amazon.com/Grokking-Reinforcement-Learning-Miguel-Morales/dp/1617295450/)\n",
      "\n",
      "Dive into Deep Learning\n",
      "\n",
      "[Deep Reinforcement Learning Hands-On](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998/)\n",
      "\n",
      "[An Algorithmic Perspective on Imitation Learning](https://arxiv.org/abs/1811.06711)\n",
      "\n",
      "[Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems](https://arxiv.org/abs/2005.01643)\n",
      "\n",
      "Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs\n",
      "\n",
      "[Reinforcement Learning](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/)\n",
      "\n",
      "[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)\n",
      "\n",
      "[Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)\n",
      "\n",
      "[Deep Learning with Python](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/)\n",
      "Chunk 7 - \n",
      "Quantum Computing\n",
      "\n",
      "[Quantum Computing Explained](https://www.amazon.com/Quantum-Computing-Explained-David-McMahon/dp/0470096993/)\n",
      "\n",
      "Quantum Linear Systems Algorithms: A Primer\n",
      "\n",
      "Quantum Algorithms via Linear Algebra\n",
      "\n",
      "[Quantum Computation and Quantum Information](https://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176/)\n",
      "\n",
      "Quantum Machine Learning for Data Scientists\n",
      "\n",
      "[Quantum computing for the very curious](https://quantum.country/qcvc)\n",
      "Chunk 8 - \n",
      "Programming Craft\n",
      "Chunk 9 - \n",
      "Distributed Systems\n",
      "\n",
      "[Big Data](https://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343/)\n",
      "\n",
      "[Designing Data-Intensive Applications](https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/)\n",
      "\n",
      "[Introduction to Parallel Computing](https://www.amazon.com/Introduction-Parallel-Computing-Ananth-Grama/dp/0201648652/)\n",
      "\n",
      "[Beej's Guide to Network Programming](https://beej.us/guide/bgnet/html/)\n",
      "\n",
      "[Build Your Own Redis with C++](https://build-your-own.org/redis/#table-of-contents)\n",
      "Chunk 10 - \n",
      "Functional Programming\n",
      "\n",
      "[Category Theory for Programmers](https://www.amazon.com/Category-Theory-Programmers-Bartosz-Milewski/dp/0464243874/)\n",
      "\n",
      "[Structure and Interpretation of Computer Programs](https://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871/)\n",
      "\n",
      "[Professor Frisby's Mostly Adequate Guide to Functional Programming](https://mostly-adequate.gitbook.io/mostly-adequate-guide/)\n",
      "Chunk 11 - \n",
      "Design\n",
      "\n",
      "Dependency Injection Principle\n",
      "\n",
      "Granularity\n",
      "\n",
      "The Interface Segregation Principle\n",
      "\n",
      "The Liskov Substitution Principle\n",
      "\n",
      "The Open-Closed Principle\n",
      "\n",
      "The Single Responsibility Principle\n",
      "\n",
      "[The Algorithm Design Manual](https://www.amazon.com/Algorithm-Design-Manual-Computer-Science/dp/3030542556/)\n",
      "\n",
      "[The Little Book of Semaphores](https://greenteapress.com/semaphores/LittleBookOfSemaphores.pdf)\n",
      "Chunk 12 - \n",
      "DevOps\n",
      "\n",
      "[Release It](https://www.amazon.com/Release-Design-Deploy-Production-Ready-Software/dp/1680502395/)\n",
      "\n",
      "[Google SRE Book](https://sre.google/sre-book/table-of-contents/)\n",
      "\n",
      "[The Unix Haters Handbook](https://web.mit.edu/~simsong/www/ugh.pdf)\n",
      "Chunk 13 - \n",
      "Misc\n",
      "\n",
      "[Web Analytics: An Hour a Day](https://www.amazon.com/Web-Analytics-Hour-Avinash-Kaushik/dp/0470130652/)\n",
      "\n",
      "[Building Secure and Reliable Systems](https://www.amazon.com/Building-Secure-Reliable-Systems-Implementing/dp/1492083127/)\n",
      "\n",
      "[The Lox Language](https://craftinginterpreters.com/the-lox-language.html)\n",
      "Chunk 14 - \n",
      "Programming Languages\n",
      "\n",
      "[Core Java for the impatient](https://www.amazon.com/Core-Java-Impatient-Cay-Horstmann/dp/0138052107/)\n",
      "\n",
      "[Scala for the impatient](https://www.amazon.com/Scala-Impatient-2nd-Cay-Horstmann/dp/0134540565/)\n",
      "\n",
      "[Learning Spark](https://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624/)\n",
      "\n",
      "[Javascript: The Good Parts](https://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742/)\n",
      "\n",
      "[HTML 5 for Masterminds](https://www.amazon.com/HTML5-Masterminds-3rd-revolutionary-applications/dp/154292331X/)\n",
      "\n",
      "[Agile Principles, Patterns, and Practices in C#](https://www.amazon.com/Agile-Principles-Patterns-Practices-C/dp/0131857258/)\n",
      "\n",
      "[Accelerated C++ - Pratical Programming by Example](https://www.amazon.com/Accelerated-C-Practical-Programming-Example/dp/020170353X/)\n",
      "\n",
      "[Discovering Modern C++](https://www.amazon.com/Discovering-Modern-Depth-Peter-Gottschling/dp/0136677649/)\n",
      "\n",
      "[Effective C++ Third Edition](https://www.amazon.com/Effective-Specific-Improve-Programs-Designs/dp/0321334876/)\n",
      "\n",
      "[Effective STL](https://www.amazon.com/Effective-STL-Specific-Standard-Template/dp/0201749629/)\n",
      "\n",
      "[Practical Python Design Patterns](https://www.amazon.com/Practical-Python-Design-Patterns-Solutions-ebook/dp/B076J622TF/)\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    print(f\"Chunk {i} - \")\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Notes\n",
    "`SimpleDirectoryReader` reads all the files in the current directory, recursively if specified, and runs the file through a format-specific reader which will chunk a single file into multiple chunks. The resulting chunks are still `Document` objects. E.g., for PDFs, it will chunk the document per page, with each page getting its own Document object. With Markdowns it will chunk based on headers (I have verified H1 and H2, don't know about the rest).\n",
    "\n",
    "`FlatReader` on the other hand will simply read the file as is without any format-specific parsing. The resulting `Document` object then needs to be passed through another more specific node parser. Using the `SimpleFileNodeParser` will result in a similar behavior as `SimpleDirectoryReader`. The difference is that `SimpleDirectoryReader` will use `MarkdownReader` to read Markdowns but `SimpleFileNodeParser` will use `MarkdownNodeParser`. The node parsers are better because they save the header information in the metadata, something that the readers do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
