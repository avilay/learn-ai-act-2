{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "This notebook goes over the most common tokenizer concepts that I am likely to need in my day-to-day work. There is a really good [comprehensive tokenizer tutorial](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt) that goes over the different tokenization schemes and how to build a new tokenizer from the ground up.\n",
    "\n",
    "### Summary\n",
    "The most common way to use tokenizers for me will be -\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentences: list[str] = ...\n",
    "batch = tokenizer(sentences, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "```\n",
    "\n",
    "Most of the models use subword tokenizers. Each model comes with its own corresponding tokenizer. This is because different models (even variants of the same core architecture) have different input schemes. The tokenizer abstracts this away. The simplest way of using a tokenizer is to just `__call__` the object. This will do the following three things - \n",
    "  1. Break the sentance up into multiple tokens\n",
    "  2. Look up the index value of each resulting token from a central vocab\n",
    "  3. Prepare ancilliary data like attention mask, token type indicators, etc. needed by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1335, 5208, 2116, 1110, 5750, 1155, 1128, 1444, 131, 118, 114, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer corresponding to this particular checkpoint\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "input = tokenizer(\"Attention is indeed all you need :-)\")\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * `input_ids` are the indexes of the tokens. Given this is a subword tokenizer, the number of tokens is more than the number of words. \n",
    "  * `token_type_ids` are used to indicate sentence boundaries, e.g., if there are two sentences that go into the model, then the first sentence will have a mask of `0`, the second will have a mask of `1`. \n",
    "  * `attention_mask` is the mask that tells the model which tokens to ignore. The ones masked with `0` are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 119, 102, 1335, 5208, 2116, 1110, 5750, 1155, 1128, 1444, 131, 118, 114, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two sentences are part of the same instance\n",
    "sentence_1 = \"Using a Transformer network is simple.\"\n",
    "sentence_2 = \"Attention is indeed all you need :-)\"\n",
    "tokenizer(sentence_1, sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can save the tokenizer locally by calling `tokenizer.save_pretrained(\"/local/path\")`\n",
    "\n",
    "To see the individual steps in action, I can use the various `tokenizer` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At', '##ten', '##tion', 'is', 'indeed', 'all', 'you', 'need', ':', '-', ')']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = tokenizer.tokenize(\"Attention is indeed all you need :-)\")\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1335, 5208, 2116, 1110, 5750, 1155, 1128, 1444, 131, 118, 114]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back from token IDs to the full sentance is also pretty easy -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Attention is indeed all you need : - ) [SEP]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[CLS]` and `[SEP]` are two special tokens that this particular BERT model was trained with. `[CLS]` is the classification task output. `[SEP]` is simply the token that separates two sentances for two-sentence inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Using a Transformer network is simple. [SEP] Attention is indeed all you need : - ) [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(sentence_1, sentence_2)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Selection of Tokenizer\n",
    "Instead of using the concrete tokenizer - `BertTokenizer` in this case, HF has a convenience class called `AutoTokenizer` that will get the right tokenizer based on the model name. This is a pretty standard pattern in HF APIs, where there is a concrete class for a particular model family, and then there are various auto-classes that will automatically choose the correct concrete implementation without the user having to worry about it. In most apps, I'll probably end up using the auto- family of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(type(tokenizer))\n",
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "Automatic tokenizer will usually select a fast variant of the tokenizer. Tokenizers can be fast and efficient when operating on a batch instead of individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7993, 25267, 1110, 3123, 106, 102], [101, 1335, 5208, 2116, 1110, 5750, 1155, 1128, 1444, 131, 118, 114, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Using Transformers is easy!\",\n",
    "    \"Attention is indeed all you need :-)\"\n",
    "]\n",
    "batch = tokenizer(sentences)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the batch - 2\n",
      "Number of tokens in the first instance - 7\n",
      "Number of tokens in the second instance - 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of instances in the batch - {len(batch['input_ids'])}\")\n",
    "print(f\"Number of tokens in the first instance - {len(batch['input_ids'][0])}\")\n",
    "print(f\"Number of tokens in the second instance - {len(batch['input_ids'][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Using Transformers is easy! [SEP]\n",
      "[CLS] Attention is indeed all you need : - ) [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "print(tokenizer.decode(batch[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where we have two sentences in the same instance, we can batch it by passing two lists to the tokenizer. The resulting batch will consist of one setence from each of input lists, i.e., the input is zipped and then tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7993, 25267, 1110, 3123, 106, 102, 2098, 1136, 1115, 1662, 1106, 1329, 25267, 106, 102], [101, 1335, 5208, 2116, 1110, 5750, 1155, 1128, 1444, 131, 118, 114, 102, 1192, 1274, 112, 189, 1444, 1625, 1167, 1190, 2209, 131, 118, 114, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_1 = [\n",
    "    \"Using Transformers is easy!\",\n",
    "    \"Attention is indeed all you need :-)\"    \n",
    "]\n",
    "\n",
    "sentences_2 = [\n",
    "    \"Its not that hard to use Transformers!\",\n",
    "    \"You don't need anything more than attention :-)\"\n",
    "]\n",
    "batch = tokenizer(sentences_1, sentences_2)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Using Transformers is easy! [SEP] Its not that hard to use Transformers! [SEP]\n",
      "[CLS] Attention is indeed all you need : - ) [SEP] You don't need anything more than attention : - ) [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "print(tokenizer.decode(batch[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens as Tensors\n",
    "So far we have seen that the `input_ids`, `attention_mask`, etc. are being retunred as a list of integers. But before we pass these to a model we'll need to convert these to tensors, `t.tensor(batch[\"input_ids\"])`, but this can get tedious to recreate the dict. HF has a convenience param called `return_tensors` where we can specify whether we want PyTorch, Tensorflow, or numpy tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7993,   170, 25267,  1110,  3123,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = tokenizer(\"Using a Transformers is easy!\", return_tensors=\"pt\")\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(toks[\"input_ids\"].shape)\n",
    "print(toks[\"token_type_ids\"].shape)\n",
    "print(toks[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding and Truncating\n",
    "\n",
    "A lot of models have a max number of tokens that they can take. It is possible to tell the tokenizer to clip or pad the tokens to confirm to this max size. I can also specify my own max size for each batch of tokens that the tokenizer generates. This is useful when I want each batch to have the same size but am ok with different batches having different sizes. Padding/truncating is a requirement when I want the tokenizer to return tensors, because tensors cannot have different sized rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 7\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"One ring to rule them all, one ring to find them, one ring to bring them all and in the darkness bind them.\",\n",
    "    \"You shall not pass!\"    \n",
    "]\n",
    "batch = tokenizer(sentences)\n",
    "print(len(batch[\"input_ids\"][0]), len(batch[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR MESSAGE:  Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer(sentences, return_tensors=\"pt\")\n",
    "except ValueError as ve:\n",
    "    print(\"ERROR MESSAGE: \", ve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(sentences, padding=True, truncation=True)\n",
    "print(len(batch[\"input_ids\"][0]), len(batch[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(sentences, padding=True, truncation=True, max_length=12)\n",
    "print(len(batch[\"input_ids\"][0]), len(batch[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1448, 3170, 1106,  102],\n",
       "        [ 101, 1192, 4103, 1136,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences, padding=True, truncation=True, max_length=5, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast Tokenizers\n",
    "HF has a bunch of tokenizers that are backed by some sort of Rust tokenization lib. This makes them really fast when processing batches. There are other tokenizers that are pure Python and they are very slow. There are two ways of figuring out whether a tokenizer is fast or not -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.is_fast)\n",
    "print(batch.is_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1422, 1271, 1110, 138, 23909, 1183, 1105, 146, 1821, 1303, 1106, 1788, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"My name is Avilay and I am here to program!\"\n",
    "encoded = tokenizer(input)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to see what tokens the ids map to. \n",
    "\n",
    "The first one is to `decode` the ids. The output in this case is the original text along with any special tokens that were inserted. This means that characters like the exclamation point, which is actually a seperate token is stuck back to the last word like it appeared in the original senetence. Similarly, the word \"Avilay\" is actually three tokens, but they are all merged together back as in the original sentence.\n",
    "\n",
    "The second way is to calle the `.tokens()` method on the output. This will give me a list of actual tokens that the tokenizer broke this sentence into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] My name is Avilay and I am here to program! [SEP]'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'A',\n",
       " '##vila',\n",
       " '##y',\n",
       " 'and',\n",
       " 'I',\n",
       " 'am',\n",
       " 'here',\n",
       " 'to',\n",
       " 'program',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a convenience method that tells me which original word each token maps to. I can see that \"Avilay\" is broken down into three tokens \"A\", \"##vila\", and \"##y\". So these three tokens will map to the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another convenience API that tells me which word was at idx 3. This API gives me the [start, end) indexes of the original sentence that this word idx maps to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Avilay'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoded.word_to_chars(3)\n",
    "input[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
