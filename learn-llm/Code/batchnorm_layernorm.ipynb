{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm\n",
    "Batch norm takes a batch of $m$ instances with $n$ features and calculates the mean and standard deviation of each feature in the batch, i.e., the rows in the input matrix. It then \"normalizes\" the input by subtracting the mean and dividing by the standard deviation. It further scales this normalized matrix by a learned scalar $\\gamma$ and shifts by another learned scalar $\\beta$.\n",
    "\n",
    "Lets have our input matrix -\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\\leftarrow \\mathbf x_1 \\rightarrow \\\\\n",
    "\\leftarrow \\mathbf x_2 \\rightarrow \\\\\n",
    "\\leftarrow \\mathbf x_3 \\rightarrow \\\\\n",
    "\\leftarrow \\mathbf x_4 \\rightarrow \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now calcualte the rowwise mean and standard deviation -\n",
    "$$\n",
    "\\mathbf \\mu = mean(\\mathbf x_1, \\mathbf x_2, \\mathbf x_3, \\mathbf x_4) \\\\\n",
    "\\mathbf \\sigma = std(\\mathbf x_1, \\mathbf x_2, \\mathbf x_3, \\mathbf x_4)\n",
    "$$\n",
    "\n",
    "Here both mean and standard deviation are also vectors with the same number of elements as the number of columns in the input matrix.\n",
    "\n",
    "Now normalize the input matrix -\n",
    "$$\n",
    "\\overline X = \\begin{bmatrix}\n",
    "\\leftarrow (\\mathbf x_1 - \\mathbf \\mu) / \\mathbf \\sigma \n",
    "\\rightarrow \\\\\n",
    "\\leftarrow (\\mathbf x_2 - \\mathbf \\mu) / \\mathbf \\sigma \n",
    "\\rightarrow \\\\\n",
    "\\leftarrow (\\mathbf x_3 - \\mathbf \\mu) / \\mathbf \\sigma \n",
    "\\rightarrow \\\\\n",
    "\\leftarrow (\\mathbf x_4 - \\mathbf \\mu) / \\mathbf \\sigma \n",
    "\\rightarrow \\\\\n",
    "\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now scale and shfit with the learned params - \n",
    "$$\n",
    "X' = \\overline X \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "Initially $\\gamma = 1$ and $\\beta = 0$. Gradually the network learns how much to scale and shift the normalized matrix.\n",
    "\n",
    "![batchnorm](./batchnorm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see this with a concrete example -\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "0.87717015 & 0.7769747 \\\\\n",
    "0.12235527 & 0.6907834 \\\\\n",
    "0.6839817 & 0.23128869 \\\\\n",
    "0.56366396 & 0.3721697 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here is the mean of each column (i.e., rowwise mean) -\n",
    "$$\n",
    "\\mu_0 = (0.87717015 + 0.12235527 + 0.6839817 + 0.56366396) / 4 = 0.5617928 \\\\\n",
    "\\mu_1 = (0.7769747 + 0.6907834 + 0.23128869 + 0.3721697) / 4 = 0.5178041\n",
    "$$\n",
    "\n",
    "We can get this specifying `axis` as $0$ in the `np.mean` function.\n",
    "\n",
    "The standard deviation of each column can also be similarly calculated.\n",
    "$$\n",
    "\\sigma_0 = 0.27726424 \\\\\n",
    "\\sigma_1 = 0.22382565\n",
    "$$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "0.87717015 & 0.7769747 \\\\\n",
    "0.12235527 & 0.6907834 \\\\\n",
    "0.6839817 & 0.23128869 \\\\\n",
    "0.56366396 & 0.3721697 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\mu = \\begin{bmatrix}\n",
    "0.5617928 & 0.5178041\n",
    "\\end{bmatrix} \\\\\n",
    "\\sigma = \\begin{bmatrix}\n",
    "0.27726424 & 0.22382565\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now the rowwise normalization -\n",
    "$$\n",
    "\\overline X = \\begin{bmatrix}\n",
    "(0.87717015 - 0.5617928) / 0.27726424 & (0.7769747 - 0.5178041) / 0.22382565 \\\\\n",
    "(0.12235527 - 0.5617928) / 0.27726424 & (0.6907834 - 0.5178041) / 0.22382565 \\\\\n",
    "(0.6839817 - 0.5617928) / 0.27726424 & (0.23128869 - 0.5178041) / 0.22382565 \\\\\n",
    "(0.56366396 - 0.5617928) / 0.27726424 & (0.3721697 - 0.5178041) / 0.22382565 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0.87717015, 0.7769747 ],\n",
    "    [0.12235527, 0.6907834 ],\n",
    "    [0.6839817 , 0.23128869],\n",
    "    [0.56366396, 0.3721697 ]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1374613257014319\n",
      "1.1579128665548388\n",
      "-1.584905179261487\n",
      "0.7728305491350078\n",
      "0.44069476828313686\n",
      "-1.28008300210454\n",
      "0.0067486524767852605\n",
      "-0.6506600114866192\n"
     ]
    }
   ],
   "source": [
    "print((0.87717015 - 0.5617928) / 0.27726424)\n",
    "print((0.7769747 - 0.5178041) / 0.22382565)\n",
    "\n",
    "print((0.12235527 - 0.5617928) / 0.27726424)\n",
    "print((0.6907834 - 0.5178041) / 0.22382565)\n",
    "\n",
    "print((0.6839817 - 0.5617928) / 0.27726424)\n",
    "print((0.23128869 - 0.5178041) / 0.22382565)\n",
    "\n",
    "print((0.56366396 - 0.5617928) / 0.27726424)\n",
    "print((0.3721697 - 0.5178041) / 0.22382565)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5617928 0.5178041] [0.27726424 0.22382565]\n"
     ]
    }
   ],
   "source": [
    "x_mean = np.mean(x, axis=0)\n",
    "x_std = np.std(x, axis=0)\n",
    "print(x_mean, x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1374613 ,  1.1579129 ],\n",
       "       [-1.5849051 ,  0.77283055],\n",
       "       [ 0.44069487, -1.2800831 ],\n",
       "       [ 0.00674868, -0.6506599 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = (x - x_mean) / x_std\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1374,  1.1578],\n",
       "        [-1.5848,  0.7728],\n",
       "        [ 0.4407, -1.2800],\n",
       "        [ 0.0067, -0.6506]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.BatchNorm1d(x.shape[-1])(t.from_numpy(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Norm\n",
    "Instead of normalizing instance vectors across the batch, layer norm normalizes each element of the instance across that instance only. It does not care about the batch at all. The learned scale and shift is as before.\n",
    "\n",
    "Lets have our input matrix -\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} & x_{14} \\\\\n",
    "x_{21} & x_{22} & x_{33} & x_{44} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now the mean and standard deviation are calculated for each instance independently -\n",
    "$$\n",
    "\\mu_1 = mean(x_{11}, x_{12}, x_{13}, x_{14}) \\\\\n",
    "\\sigma_1 = std(x_{11}, x_{12}, x_{13}, x_{14}) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_2 = mean(x_{21}, x_{22}, x_{33}, x_{44}) \\\\\n",
    "\\sigma_2 = std(x_{21}, x_{22}, x_{33}, x_{44})\n",
    "$$\n",
    "\n",
    "Here the mean and standard deviation for each instance are scalars.\n",
    "\n",
    "Now normalize the input matrix -\n",
    "$$\n",
    "\\overline X = \\begin{bmatrix}\n",
    "(x_{11} - \\mu_1)/\\sigma_1 & (x_{12} - \\mu_1)/\\sigma_1 & (x_{13} - \\mu_1)/\\sigma_1 & (x_{14} - \\mu_1)/\\sigma_1 \\\\\n",
    "(x_{21} - \\mu_2)/\\sigma_2 & (x_{22} - \\mu_2)/\\sigma_2 & (x_{23} - \\mu_2)/\\sigma_2 & (x_{24} - \\mu_2)/\\sigma_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And then scale and shift as before -\n",
    "$$\n",
    "X' = \\overline X \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "![layernorm](./layernorm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76992553, 0.00166408, 0.5785207 , 0.7359749 ],\n",
       "       [0.55730516, 0.5911572 , 0.5388567 , 0.5622644 ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rng.random((2, 4)).astype(np.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52152133 0.5623959 ] [0.30870515 0.0187566 ]\n"
     ]
    }
   ],
   "source": [
    "x_mean = np.mean(x, axis=1)\n",
    "x_std = np.std(x, axis=1)\n",
    "print(x_mean, x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8046649 , -1.6839927 ,  0.18464021,  0.6946874 ],\n",
       "       [-0.2714092 ,  1.5333978 , -1.2549816 , -0.00701022]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = (x - x_mean.reshape(-1, 1)) / x_std.reshape(-1, 1)\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8046, -1.6839,  0.1846,  0.6947],\n",
       "        [-0.2676,  1.5121, -1.2375, -0.0069]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.nn.LayerNorm(4)(t.from_numpy(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
