{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "## Small Grid World\n",
    "Below is the definition of a small grid world as described in Silver's lecture.\n",
    "  * Undiscounted episodic MDP ($\\gamma = 1$)\n",
    "  * 4x4 grid\n",
    "  * 2 terminal states: (0, 0) and (3, 3)\n",
    "  * Actions leading out of the grid leave the state unchanged\n",
    "  * Reward is -1 until the terminal state is reached\n",
    "  * Agent follows uniform random policy\n",
    "\n",
    "$$\n",
    "\\pi(n \\vert \\cdot) = \\pi(e \\vert \\cdot) = \\pi(s \\vert \\cdot) = \\pi(w \\vert \\cdot) = \\frac{1}{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-Action Value Iteration\n",
    "In the `state_values` notebook, I computed everything as a function of state values, i.e., other state values as well as state-action values were computed as function of state values. In this notebook, I'll compute everything as a function of state-action values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q(s, a) &= r(s, a) + \\gamma \\mathbb E_{\\pi, P}\\left[ q(s', a') \\right] \\\\\n",
    "&= r(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\vert s, a) \\sum_{a' \\in A} \\pi(a' \\vert s') \\; q(s', a')\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "I've never seen this anywhere, but I am guessing it will be possible to calculate q-values using dynamic programming as well.\n",
    "\n",
    "$$\n",
    "q_{k+1}(s, a) = r(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\vert s, a) \\sum_{a' \\in A} \\pi(a' \\vert s') \\; q_k(s', a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from small_grid_world import SmallGridWorld, State, Action, Policy\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "QTable = dict[tuple[State, Action], float]\n",
    "\n",
    "MAX_ITERS = 1000\n",
    "\n",
    "\n",
    "def has_converged(qvals: QTable, qvals_next: QTable) -> bool:\n",
    "    if all(v == 0 for v in qvals.values()):\n",
    "        return False\n",
    "    if all(v == 0 for v in qvals_next.values()):\n",
    "        return False\n",
    "    return np.allclose([v for v in qvals.values()], [v for v in qvals_next.values()])\n",
    "\n",
    "\n",
    "def calc_qvals(mdp: SmallGridWorld, pi: Policy) -> QTable:\n",
    "    qvals: QTable = defaultdict(float)\n",
    "    qvals_next: QTable = defaultdict(float)\n",
    "    r = mdp.reward\n",
    "    p = mdp.prob\n",
    "    γ = mdp.gamma\n",
    "    i = 0\n",
    "    while i < MAX_ITERS and not has_converged(qvals, qvals_next):\n",
    "        for s, a in product(mdp.states(), mdp.actions()):\n",
    "            qvals_next[(s, a)] = r(s, a) + γ * sum(\n",
    "                p(s_, given=(s, a))\n",
    "                * sum(pi(a_, given=s_) * qvals[(s_, a_)] for a_ in mdp.actions())\n",
    "                for s_ in mdp.states()\n",
    "            )\n",
    "        qvals = qvals_next\n",
    "        qvals_next = defaultdict(float)\n",
    "        i += 1\n",
    "    for s, a in qvals.keys():\n",
    "        qvals[(s, a)] = round(qvals[(s, a)])\n",
    "    return qvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_random_policy(mdp):\n",
    "    def policy(a, given):\n",
    "        return 0 if mdp.is_terminal(given) else 0.25\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q((0, 1), ↑) = -15\n",
      "Q((0, 1), ↓) = -19\n",
      "Q((0, 1), ←) = -1\n",
      "Q((0, 1), →) = -21\n",
      "Q((0, 2), ↑) = -21\n",
      "Q((0, 2), ↓) = -21\n",
      "Q((0, 2), ←) = -15\n",
      "Q((0, 2), →) = -23\n",
      "Q((0, 3), ↑) = -23\n",
      "Q((0, 3), ↓) = -21\n",
      "Q((0, 3), ←) = -21\n",
      "Q((0, 3), →) = -23\n",
      "Q((1, 0), ↑) = -1\n",
      "Q((1, 0), ↓) = -21\n",
      "Q((1, 0), ←) = -15\n",
      "Q((1, 0), →) = -19\n",
      "Q((1, 1), ↑) = -15\n",
      "Q((1, 1), ↓) = -21\n",
      "Q((1, 1), ←) = -15\n",
      "Q((1, 1), →) = -21\n",
      "Q((1, 2), ↑) = -21\n",
      "Q((1, 2), ↓) = -19\n",
      "Q((1, 2), ←) = -19\n",
      "Q((1, 2), →) = -21\n",
      "Q((1, 3), ↑) = -23\n",
      "Q((1, 3), ↓) = -15\n",
      "Q((1, 3), ←) = -21\n",
      "Q((1, 3), →) = -21\n",
      "Q((2, 0), ↑) = -15\n",
      "Q((2, 0), ↓) = -23\n",
      "Q((2, 0), ←) = -21\n",
      "Q((2, 0), →) = -21\n",
      "Q((2, 1), ↑) = -19\n",
      "Q((2, 1), ↓) = -21\n",
      "Q((2, 1), ←) = -21\n",
      "Q((2, 1), →) = -19\n",
      "Q((2, 2), ↑) = -21\n",
      "Q((2, 2), ↓) = -15\n",
      "Q((2, 2), ←) = -21\n",
      "Q((2, 2), →) = -15\n",
      "Q((2, 3), ↑) = -21\n",
      "Q((2, 3), ↓) = -1\n",
      "Q((2, 3), ←) = -19\n",
      "Q((2, 3), →) = -15\n",
      "Q((3, 0), ↑) = -21\n",
      "Q((3, 0), ↓) = -23\n",
      "Q((3, 0), ←) = -23\n",
      "Q((3, 0), →) = -21\n",
      "Q((3, 1), ↑) = -21\n",
      "Q((3, 1), ↓) = -21\n",
      "Q((3, 1), ←) = -23\n",
      "Q((3, 1), →) = -15\n",
      "Q((3, 2), ↑) = -19\n",
      "Q((3, 2), ↓) = -15\n",
      "Q((3, 2), ←) = -21\n",
      "Q((3, 2), →) = -1\n"
     ]
    }
   ],
   "source": [
    "mdp = SmallGridWorld()\n",
    "qvals = calc_qvals(mdp, generate_uniform_random_policy(mdp))\n",
    "for s, a in product(mdp.states(), mdp.actions()):\n",
    "    if qvals[(s, a)] != 0:\n",
    "        print(f\"Q({s}, {a}) = {qvals[(s, a)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qval(mdp, v, s, a):\n",
    "    r = mdp.reward\n",
    "    p = mdp.prob\n",
    "    γ = mdp.gamma\n",
    "\n",
    "    return r(s, a) + γ * sum(p(s_, given=(s, a)) * v[*s_] for s_ in mdp.states())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array(\n",
    "    [\n",
    "        [0.0, -14.0, -20.0, -22.0],\n",
    "        [-14.0, -18.0, -20.0, -20.0],\n",
    "        [-20.0, -20.0, -18.0, -14.0],\n",
    "        [-22.0, -20.0, -14.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "for s, a in product(mdp.states(), mdp.actions()):\n",
    "    expected_qval = qval(mdp, vals, s, a)\n",
    "    actual_qval = qvals[(s, a)]\n",
    "    assert (\n",
    "        expected_qval == actual_qval\n",
    "    ), f\"Expected {expected_qval} but got {actual_qval} for Q({s}, {a})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal State-Action Values\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_*(s, a) &= r(s, a) + \\gamma \\mathbb E_P \\left[ \\underset{a'}{max}(q_*(s', a')) \\right] \\\\\n",
    "&= r(s, a) + \\gamma \\sum_{s' \\in S} P\\left(s' \\vert s, a\\right) \\; \\underset{a'}{max}(q_*(s', a')) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Applying dynamic programming -\n",
    "\n",
    "$$\n",
    "q^*_{k+1} = r(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\vert s, a) \\; \\underset{a'}{max}(q^*_k(s', a'))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_qvals(mdp: SmallGridWorld) -> QTable:\n",
    "    qvals: QTable = defaultdict(float)\n",
    "    qvals_next: QTable = defaultdict(float)\n",
    "    r = mdp.reward\n",
    "    p = mdp.prob\n",
    "    γ = mdp.gamma\n",
    "    i = 0\n",
    "    while i < MAX_ITERS and not has_converged(qvals, qvals_next):\n",
    "        for s, a in product(mdp.states(), mdp.actions()):\n",
    "            qvals_next[(s, a)] = r(s, a) + γ * sum(\n",
    "                p(s_, given=(s, a)) * max(qvals[(s_, a_)] for a_ in mdp.actions())\n",
    "                for s_ in mdp.states()\n",
    "            )\n",
    "        qvals = qvals_next\n",
    "        qvals_next = defaultdict(float)\n",
    "        i += 1\n",
    "    for s, a in qvals.keys():\n",
    "        qvals[(s, a)] = round(qvals[(s, a)])\n",
    "    return qvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q*((0, 1), ↑) = -2\n",
      "Q*((0, 1), ↓) = -3\n",
      "Q*((0, 1), ←) = -1\n",
      "Q*((0, 1), →) = -3\n",
      "Q*((0, 2), ↑) = -3\n",
      "Q*((0, 2), ↓) = -4\n",
      "Q*((0, 2), ←) = -2\n",
      "Q*((0, 2), →) = -4\n",
      "Q*((0, 3), ↑) = -4\n",
      "Q*((0, 3), ↓) = -3\n",
      "Q*((0, 3), ←) = -3\n",
      "Q*((0, 3), →) = -4\n",
      "Q*((1, 0), ↑) = -1\n",
      "Q*((1, 0), ↓) = -3\n",
      "Q*((1, 0), ←) = -2\n",
      "Q*((1, 0), →) = -3\n",
      "Q*((1, 1), ↑) = -2\n",
      "Q*((1, 1), ↓) = -4\n",
      "Q*((1, 1), ←) = -2\n",
      "Q*((1, 1), →) = -4\n",
      "Q*((1, 2), ↑) = -3\n",
      "Q*((1, 2), ↓) = -3\n",
      "Q*((1, 2), ←) = -3\n",
      "Q*((1, 2), →) = -3\n",
      "Q*((1, 3), ↑) = -4\n",
      "Q*((1, 3), ↓) = -2\n",
      "Q*((1, 3), ←) = -4\n",
      "Q*((1, 3), →) = -3\n",
      "Q*((2, 0), ↑) = -2\n",
      "Q*((2, 0), ↓) = -4\n",
      "Q*((2, 0), ←) = -3\n",
      "Q*((2, 0), →) = -4\n",
      "Q*((2, 1), ↑) = -3\n",
      "Q*((2, 1), ↓) = -3\n",
      "Q*((2, 1), ←) = -3\n",
      "Q*((2, 1), →) = -3\n",
      "Q*((2, 2), ↑) = -4\n",
      "Q*((2, 2), ↓) = -2\n",
      "Q*((2, 2), ←) = -4\n",
      "Q*((2, 2), →) = -2\n",
      "Q*((2, 3), ↑) = -3\n",
      "Q*((2, 3), ↓) = -1\n",
      "Q*((2, 3), ←) = -3\n",
      "Q*((2, 3), →) = -2\n",
      "Q*((3, 0), ↑) = -3\n",
      "Q*((3, 0), ↓) = -4\n",
      "Q*((3, 0), ←) = -4\n",
      "Q*((3, 0), →) = -3\n",
      "Q*((3, 1), ↑) = -4\n",
      "Q*((3, 1), ↓) = -3\n",
      "Q*((3, 1), ←) = -4\n",
      "Q*((3, 1), →) = -2\n",
      "Q*((3, 2), ↑) = -3\n",
      "Q*((3, 2), ↓) = -2\n",
      "Q*((3, 2), ←) = -3\n",
      "Q*((3, 2), →) = -1\n"
     ]
    }
   ],
   "source": [
    "qvals_star = optimal_qvals(mdp)\n",
    "for s, a in product(mdp.states(), mdp.actions()):\n",
    "    if qvals_star[(s, a)] != 0:\n",
    "        print(f\"Q*({s}, {a}) = {qvals_star[(s, a)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array(\n",
    "    [\n",
    "        [0.0, -1.0, -2.0, -3.0],\n",
    "        [-1.0, -2.0, -3.0, -2.0],\n",
    "        [-2.0, -3.0, -2.0, -1.0],\n",
    "        [-3.0, -2.0, -1.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "for s, a in product(mdp.states(), mdp.actions()):\n",
    "    expected_qval = qval(mdp, vals, s, a)\n",
    "    actual_qval = qvals_star[(s, a)]\n",
    "    assert (\n",
    "        expected_qval == actual_qval\n",
    "    ), f\"Expected {expected_qval} but got {actual_qval} for Q({s}, {a})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from small_grid_world import argmax\n",
    "\n",
    "\n",
    "def greedy_policy(\n",
    "    mdp: SmallGridWorld, q_star: QTable, s: State, dbg=False\n",
    ") -> Action | None:\n",
    "    if mdp.is_terminal(s):\n",
    "        return None\n",
    "\n",
    "    best_actions = argmax(mdp.actions(), key=lambda a: q_star[(s, a)])\n",
    "    best_action = best_actions[0]\n",
    "    if dbg:\n",
    "        if len(best_actions) > 1:\n",
    "            best_actions_str = \", \".join(str(a) for a in best_actions)\n",
    "            print(\n",
    "                f\"\\tDEBUG: pi({s}): Got {len(best_actions)} best actions: {best_actions_str}, choosing {best_action}\"\n",
    "            )\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_policy = {\n",
    "    (0, 1): \"←\",\n",
    "    (0, 2): \"←\",\n",
    "    (0, 3): \"↓\",\n",
    "    (1, 0): \"↑\",\n",
    "    (1, 1): \"↑\",\n",
    "    (1, 2): \"↓\",\n",
    "    (1, 3): \"↓\",\n",
    "    (2, 0): \"↑\",\n",
    "    (2, 1): \"↑\",\n",
    "    (2, 2): \"↓\",\n",
    "    (2, 3): \"↓\",\n",
    "    (3, 0): \"↑\",\n",
    "    (3, 1): \"→\",\n",
    "    (3, 2): \"→\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tDEBUG: pi((0, 3)): Got 2 best actions: ↓, ←, choosing ↓\n",
      "\tDEBUG: pi((1, 1)): Got 2 best actions: ↑, ←, choosing ↑\n",
      "\tDEBUG: pi((1, 2)): Got 4 best actions: ↑, ↓, ←, →, choosing ↑\n",
      "Expected ↓ but got ↑ for pi((1, 2))\n",
      "\tDEBUG: pi((2, 1)): Got 4 best actions: ↑, ↓, ←, →, choosing ↑\n",
      "\tDEBUG: pi((2, 2)): Got 2 best actions: ↓, →, choosing ↓\n",
      "\tDEBUG: pi((3, 0)): Got 2 best actions: ↑, →, choosing ↑\n"
     ]
    }
   ],
   "source": [
    "for s in mdp.states():\n",
    "    if mdp.is_terminal(s):\n",
    "        continue\n",
    "    expected_a = expected_policy[s]\n",
    "    actual_a = str(greedy_policy(mdp, qvals_star, s, dbg=True))\n",
    "    if expected_a != actual_a:\n",
    "        print(f\"Expected {expected_a} but got {actual_a} for pi({s})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
