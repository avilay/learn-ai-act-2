{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "## Small Grid World\n",
    "Below is the definition of a small grid world as described in Silver's lecture.\n",
    "  * Undiscounted episodic MDP ($\\gamma = 1$)\n",
    "  * 4x4 grid\n",
    "  * 2 terminal states: (0, 0) and (3, 3)\n",
    "  * Actions leading out of the grid leave the state unchanged\n",
    "  * Reward is -1 until the terminal state is reached\n",
    "  * Agent follows uniform random policy\n",
    "\n",
    "$$\n",
    "\\pi(n \\vert \\cdot) = \\pi(e \\vert \\cdot) = \\pi(s \\vert \\cdot) = \\pi(w \\vert \\cdot) = \\frac{1}{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "### Belmann's Equation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) &= \\mathbb E_{\\pi, P} \\left[ r(s, a) + \\gamma P(s' \\vert s, a) v(s') \\right] \\\\\n",
    "&= \\sum_{a \\in \\mathcal A} \\pi(a \\vert s) \\left\\{ r(s, a) + \\gamma \\sum_{s' \\in \\mathcal S} P(s' \\vert s, a) v(s') \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Vectorized form of the above equation is -\n",
    "$$\n",
    "\\mathbf v_{\\pi} = \\mathbf r + \\gamma P_{\\pi} \\mathbf v_{\\pi}\n",
    "$$\n",
    "\n",
    "Two ways to solve this - \n",
    "  (i) Vectorized Form\n",
    "  (ii) Dynamic Programming\n",
    "\n",
    "### Vectorized Form\n",
    "$$\n",
    "\\mathbf v_{\\pi} = (I - \\gamma P_{\\pi})^{-1} \\mathbf r\n",
    "$$\n",
    "\n",
    "### Dynamic Programming\n",
    "$$\n",
    "v_{k+1}(s) = \\sum_{a \\in \\mathcal A} \\pi(a \\vert s) \\left\\{ r(s, a) + \\gamma \\sum_{s' \\in \\mathcal S} P(s' \\vert s, a) v_k(s') \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got convergence after 158 iters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knowing the full MDP, I can calculate the state values for this specific problem.\n",
    "n_iters = 2048\n",
    "values = np.zeros((4, 4), dtype=np.float32)\n",
    "values_next = np.zeros((4, 4), dtype=np.float32)\n",
    "\n",
    "up = lambda r, c: (max(0, r - 1), c)\n",
    "down = lambda r, c: (min(3, r + 1), c)\n",
    "left = lambda r, c: (r, max(0, c - 1))\n",
    "right = lambda r, c: (r, min(3, c + 1))\n",
    "\n",
    "for i in range(n_iters):\n",
    "    for r in range(4):\n",
    "        for c in range(4):\n",
    "            if (r == 0 and c == 0) or (r == 3 and c == 3):\n",
    "                continue\n",
    "            values_next[r, c] = -1 + 0.25 * (\n",
    "                values[up(r, c)]\n",
    "                + values[down(r, c)]\n",
    "                + values[left(r, c)]\n",
    "                + values[right(r, c)]\n",
    "            )\n",
    "    if np.allclose(values_next, values):\n",
    "        print(f\"Got convergence after {i} iters.\")\n",
    "        break\n",
    "    values = values_next\n",
    "    values_next = np.zeros((4, 4), dtype=np.float32)\n",
    "\n",
    "np.rint(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from small_grid_world import (\n",
    "    Policy,\n",
    "    SmallGridWorld,\n",
    "    State,\n",
    "    Grid,\n",
    "    Action,\n",
    "    argmax,\n",
    ")\n",
    "\n",
    "MAX_ITERS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_v_next(mdp: SmallGridWorld, v: Grid, pi: Policy, s: State) -> float:\n",
    "    r = mdp.reward\n",
    "    p = mdp.prob\n",
    "    γ = mdp.gamma\n",
    "\n",
    "    v_ = 0.0\n",
    "    for a in mdp.actions():\n",
    "        q = r(s, a) + γ * sum(p(s_, given=(s, a)) * v[s_] for s_ in mdp.states())\n",
    "        v_ += pi(a, given=s) * q\n",
    "    return v_\n",
    "\n",
    "\n",
    "def calc_svals(mdp: SmallGridWorld, pi: Policy) -> Grid:\n",
    "    v = Grid(4, random=True)\n",
    "    v_next = Grid(4)\n",
    "    i = 0\n",
    "    while i < MAX_ITERS and not v.close(v_next):\n",
    "        v.copy(v_next)\n",
    "        v_next.clear()\n",
    "        for s in mdp.states():\n",
    "            v_next[s] = _calc_v_next(mdp, v, pi, s)\n",
    "        i += 1\n",
    "    print(f\"Converged after {i} iterations.\")\n",
    "    v.round()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_random_policy(mdp):\n",
    "    def policy(a, given):\n",
    "        return 0 if mdp.is_terminal(given) else 0.25\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 159 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_grid = SmallGridWorld()\n",
    "v = calc_svals(small_grid, generate_uniform_random_policy(small_grid))\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qval(mdp: SmallGridWorld, v: Grid, s: State, a: Action) -> float:\n",
    "    r = mdp.reward\n",
    "    p = mdp.prob\n",
    "    γ = mdp.gamma\n",
    "\n",
    "    return r(s, a) + γ * sum(p(s_, given=(s, a)) * v[s_] for s_ in mdp.states())\n",
    "\n",
    "\n",
    "def generate_greedy_policy(mdp: SmallGridWorld, v: Grid) -> Policy:\n",
    "    def policy(action: Action, given: State, dbg=False) -> float:\n",
    "        s = given\n",
    "        q_s = partial(qval, mdp, v, s)\n",
    "\n",
    "        if mdp.is_terminal(s):\n",
    "            return 0.0\n",
    "\n",
    "        best_actions = argmax(mdp.actions(), key=lambda a: q_s(a))\n",
    "        best_action = best_actions[0]\n",
    "        if dbg and np.array_equal(action, best_action):\n",
    "            if len(best_actions) > 1:\n",
    "                best_actions_str = \", \".join(str(a) for a in best_actions)\n",
    "                print(\n",
    "                    f\"\\tDEBUG: pi(action={action}, state={s}): Got {len(best_actions)} best actions: {best_actions_str}, choosing {best_action}\"\n",
    "                )\n",
    "\n",
    "        if action == best_action:\n",
    "            return 1.0\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "π((0, 1)) = ←\n",
      "π((0, 2)) = ←\n",
      "\tDEBUG: pi(action=↓, state=(0, 3)): Got 2 best actions: ↓, ←, choosing ↓\n",
      "π((0, 3)) = ↓\n",
      "π((1, 0)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(1, 1)): Got 2 best actions: ↑, ←, choosing ↑\n",
      "π((1, 1)) = ↑\n",
      "\tDEBUG: pi(action=↓, state=(1, 2)): Got 2 best actions: ↓, ←, choosing ↓\n",
      "π((1, 2)) = ↓\n",
      "π((1, 3)) = ↓\n",
      "π((2, 0)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(2, 1)): Got 2 best actions: ↑, →, choosing ↑\n",
      "π((2, 1)) = ↑\n",
      "\tDEBUG: pi(action=↓, state=(2, 2)): Got 2 best actions: ↓, →, choosing ↓\n",
      "π((2, 2)) = ↓\n",
      "π((2, 3)) = ↓\n",
      "\tDEBUG: pi(action=↑, state=(3, 0)): Got 2 best actions: ↑, →, choosing ↑\n",
      "π((3, 0)) = ↑\n",
      "π((3, 1)) = →\n",
      "π((3, 2)) = →\n"
     ]
    }
   ],
   "source": [
    "gp = generate_greedy_policy(small_grid, v)\n",
    "for s in small_grid.states():\n",
    "    for a in small_grid.actions():\n",
    "        prob = gp(a, s, dbg=True)\n",
    "        if prob == 1.0:\n",
    "            print(f\"π({s}) = {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(mdp: SmallGridWorld) -> tuple[Grid, Policy]:\n",
    "    v = Grid(4)\n",
    "    pi = generate_uniform_random_policy(mdp)\n",
    "    v_pi = calc_svals(mdp, pi)\n",
    "\n",
    "    while not v_pi.close(v):\n",
    "        v.copy(v_pi)\n",
    "        pi = generate_greedy_policy(mdp, v)\n",
    "        v_pi = calc_svals(mdp, pi)\n",
    "\n",
    "    return v_pi, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 159 iterations.\n",
      "Converged after 4 iterations.\n",
      "Converged after 4 iterations.\n"
     ]
    }
   ],
   "source": [
    "v, pi = optimal_policy(small_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "π((0, 1)) = ←\n",
      "π((0, 2)) = ←\n",
      "\tDEBUG: pi(action=↓, state=(0, 3)): Got 2 best actions: ↓, ←, choosing ↓\n",
      "π((0, 3)) = ↓\n",
      "π((1, 0)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(1, 1)): Got 2 best actions: ↑, ←, choosing ↑\n",
      "π((1, 1)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(1, 2)): Got 4 best actions: ↑, ↓, ←, →, choosing ↑\n",
      "π((1, 2)) = ↑\n",
      "π((1, 3)) = ↓\n",
      "π((2, 0)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(2, 1)): Got 4 best actions: ↑, ↓, ←, →, choosing ↑\n",
      "π((2, 1)) = ↑\n",
      "\tDEBUG: pi(action=↓, state=(2, 2)): Got 2 best actions: ↓, →, choosing ↓\n",
      "π((2, 2)) = ↓\n",
      "π((2, 3)) = ↓\n",
      "\tDEBUG: pi(action=↑, state=(3, 0)): Got 2 best actions: ↑, →, choosing ↑\n",
      "π((3, 0)) = ↑\n",
      "π((3, 1)) = →\n",
      "π((3, 2)) = →\n"
     ]
    }
   ],
   "source": [
    "for s in small_grid.states():\n",
    "    for a in small_grid.actions():\n",
    "        prob = pi(a, s, dbg=True)\n",
    "        if prob == 1.0:\n",
    "            print(f\"π({s}) = {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Instead of going iterating through evaluating policy and then improving it, I can directly find the optimal state values and therefore the optimal policy using the Belman's optimality equation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_*(s) &= \\underset{a}{max} \\left( r(s, a) + \\gamma E_P \\left[ v_*(s') \\right] \\right) \\\\\n",
    "&= \\underset{a}{max} \\left( r(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\vert s, a) \\; v_*(s') \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using dynamic programming I can iteratively find the solution -\n",
    "$$\n",
    "v^*_{k+1}(s) = \\underset{a}{max} \\left( r(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\vert s, a) \\; v^*_k(s') \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_svals(mdp: SmallGridWorld) -> Grid:\n",
    "    v = Grid(4, random=True)\n",
    "    v_next = Grid(4)\n",
    "    i = 0\n",
    "    while i < MAX_ITERS and not v.close(v_next):\n",
    "        v.copy(v_next)\n",
    "        v_next.clear()\n",
    "        q = partial(qval, mdp, v)\n",
    "        for s in mdp.states():\n",
    "            v_next[s] = max(q(s, a) for a in mdp.actions())\n",
    "        i += 1\n",
    "    print(f\"Converged after {i} iterations.\")\n",
    "    v.round()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 4 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_star = optimal_svals(small_grid)\n",
    "v_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "π*((0, 1)) = ←\n",
      "π*((0, 2)) = ←\n",
      "\tDEBUG: pi(action=↓, state=(0, 3)): Got 2 best actions: ↓, ←, choosing ↓\n",
      "π*((0, 3)) = ↓\n",
      "π*((1, 0)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(1, 1)): Got 2 best actions: ↑, ←, choosing ↑\n",
      "π*((1, 1)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(1, 2)): Got 4 best actions: ↑, ↓, ←, →, choosing ↑\n",
      "π*((1, 2)) = ↑\n",
      "π*((1, 3)) = ↓\n",
      "π*((2, 0)) = ↑\n",
      "\tDEBUG: pi(action=↑, state=(2, 1)): Got 4 best actions: ↑, ↓, ←, →, choosing ↑\n",
      "π*((2, 1)) = ↑\n",
      "\tDEBUG: pi(action=↓, state=(2, 2)): Got 2 best actions: ↓, →, choosing ↓\n",
      "π*((2, 2)) = ↓\n",
      "π*((2, 3)) = ↓\n",
      "\tDEBUG: pi(action=↑, state=(3, 0)): Got 2 best actions: ↑, →, choosing ↑\n",
      "π*((3, 0)) = ↑\n",
      "π*((3, 1)) = →\n",
      "π*((3, 2)) = →\n"
     ]
    }
   ],
   "source": [
    "pi_star = generate_greedy_policy(small_grid, v_star)\n",
    "for s in small_grid.states():\n",
    "    for a in small_grid.actions():\n",
    "        prob = pi_star(a, s, dbg=True)\n",
    "        if prob == 1.0:\n",
    "            print(f\"π*({s}) = {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
